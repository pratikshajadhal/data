{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8315afbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Glue Interactive Sessions Kernel\n",
      "For more information on available magic commands, please type %help in any new cell.\n",
      "\n",
      "Please view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\n",
      "Current iam_role is None\n",
      "iam_role has been set to arn:aws:iam::481237842548:role/CF-dev-TruveDevops-Glue-Role.\n"
     ]
    }
   ],
   "source": [
    "%iam_role arn:aws:iam::481237842548:role/CF-dev-TruveDevops-Glue-Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86f8f0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous region: None\n",
      "Setting new region to: us-west-2\n",
      "Region is set to: us-west-2\n"
     ]
    }
   ],
   "source": [
    "%region us-west-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "c7525825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional python modules to be included:\n",
      "s3://dev-truve-devops-05-databricks-bucketetlrawdata-wu3m2thgf3o/sstm_transformation-0.1.0-py3-none-any.whl\n",
      "Previous number of workers: 5\n",
      "Setting new number of workers to: 2\n",
      "Authenticating with profile=default\n",
      "glue_role_arn defined by user: arn:aws:iam::481237842548:role/CF-dev-TruveDevops-Glue-Role\n",
      "Trying to create a Glue session for the kernel.\n",
      "Worker Type: G.1X\n",
      "Number of Workers: 2\n",
      "Session ID: c80607c3-b44b-4b48-854a-b878afe0af8c\n",
      "Applying the following default arguments:\n",
      "--glue_kernel_version 0.31\n",
      "--enable-glue-datacatalog true\n",
      "--additional-python-modules s3://dev-truve-devops-05-databricks-bucketetlrawdata-wu3m2thgf3o/sstm_transformation-0.1.0-py3-none-any.whl\n",
      "Waiting for session c80607c3-b44b-4b48-854a-b878afe0af8c to get into ready status...\n",
      "Session c80607c3-b44b-4b48-854a-b878afe0af8c has been created\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%additional_python_modules \"s3://dev-truve-devops-05-databricks-bucketetlrawdata-wu3m2thgf3o/sstm_transformation-0.1.0-py3-none-any.whl\"\n",
    "%number_of_workers 2\n",
    "\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "  \n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f82a92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#download configuration file\n",
    "import boto3\n",
    "session = boto3.Session()\n",
    "\n",
    "s3 = session.resource('s3')\n",
    "s3.Bucket('dev-truve-devops-05-databricks-bucketetlrawdata-wu3m2thgf3o').download_file('confs/filevine/sstm.yaml', 'sstm.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8ae1f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"sstm.yaml\", \"r\") as f:\n",
    "    sstm_config = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e61f8750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'org_id: 6586\\ntsm:\\n  - name: PeopleType\\n    fields:\\n    - name: Truve_Org_ID\\n      data_type: int\\n      transform: \\n        source: internal\\n        type: key\\n    - name: Client_Org_ID\\n      data_type: int\\n      transform:\\n        source: etl\\n        type: org_id\\n    - name: People_Type_ID\\n      data_type: int\\n    - name: People_Type\\n      data_type: string\\n      transform:\\n        source: etl\\n        source_entity_type: core\\n        source_entity_name: contact\\n        source_field: personTypes\\n        type: data\\n    - name: People_Sub_Type\\n      data_type: string\\n    - name: Custom1\\n      data_type: string\\n    - name: Custom2\\n      data_type: string\\n    - name: Custom3\\n      data_type: string\\n  - name: PeopleMaster\\n    fields:\\n    - name: Truve_Org_ID\\n      data_type: int\\n      transform: \\n        source: internal\\n        type: key\\n    - name: Client_Org_ID\\n      data_type: int\\n      transform:\\n        source: etl\\n        type: org_id\\n    - name: People_ID\\n      data_type: int\\n      transform:\\n        source: etl\\n        source_entity_type: core\\n        source_entity_name: contact\\n        source_field: personId\\n        type: data    \\n    - name: People_Type_ID\\n      data_type: int\\n      transform:\\n        source: etl\\n        source_entity_type: core\\n        source_entity_name: contact\\n        source_field: personTypes\\n        type: data\\n    - name: Team_ID\\n      data_type: int\\n      transform: \\n        source: internal\\n        type: key\\n    - name: First_Name\\n      data_type: string\\n      transform:\\n        source: etl\\n        source_entity_type: core\\n        source_entity_name: contact\\n        source_field: firstName\\n        type: data\\n    - name: Middle_Name\\n      data_type: string\\n    - name: Last_Name\\n      data_type: string\\n      transform:\\n        source: etl\\n        source_entity_type: core\\n        source_entity_name: contact\\n        source_field: lastName\\n        type: data\\n    - name: Date_of_Birth\\n      data_type: date\\n    - name: Gender\\n      data_type: string\\n    - name: Custom1\\n      data_type: string\\n    - name: Custom2\\n      data_type: string\\n    - name: Custom3\\n      data_type: string    \\n  - name: ProjectMaster  \\n    fields:\\n    - name: Truve_Org_ID\\n      data_type: int\\n      transform: \\n        source: internal\\n        type: key\\n    - name: Client_Org_ID\\n      data_type: int\\n      transform:\\n        source: etl\\n        type: org_id\\n    - name: Project_ID\\n      data_type: int\\n      transform:\\n        source: etl\\n        source_entity_type: core\\n        source_entity_name: project\\n        source_field: projectId\\n        type: data\\n    - name: Project_Type_ID\\n      data_type: int\\n      transform:\\n        source: etl\\n        source_entity_type: core\\n        source_entity_name: project\\n        source_field: projectTypeId\\n        type: data\\n    - name: Is_Archived\\n      data_type: boolean\\n      transform:\\n        source: etl\\n        source_entity_type: core\\n        source_entity_name: project\\n        source_field: isArchived\\n        type: data\\n    - name: Incident_Date\\n      data_type: date\\n      transform:\\n        source: etl\\n        source_entity_type: core\\n        source_entity_name: project\\n        source_field: incidentDate\\n        type: data\\n  - name: CaseSummary\\n    fields:\\n    - name: Truve_Org_ID\\n      data_type: int\\n      transform: \\n        source: internal\\n        type: key\\n    - name: Client_Org_ID\\n      data_type: int\\n      transform:\\n        source: etl\\n        type: org_id\\n    - name: Parent_Case_ID\\n      data_type: int\\n      transform:\\n        source: etl\\n        source_entity_type: form\\n        source_entity_name: casesummary\\n        source_field: projectId\\n        type: data\\n    - name: Case_ID\\n      data_type: int\\n      transform:\\n        source: etl\\n        source_entity_type: form\\n        source_entity_name: casesummary\\n        source_field: projectId\\n        type: data\\n    - name: Case_Type_ID\\n      data_type: int\\n    - name: Case_Create_Date\\n      data_type: date\\n    - name: Date_of_Incident\\n      data_type: date\\n      transform:\\n        source: etl\\n        source_entity_type: form\\n        source_entity_name: casesummary\\n        source_field: dateOfIncident\\n        type: data\\n    - name: Case_Name\\n      data_type: string\\n    - name: Plaintiff_Full_Name\\n      data_type: string\\n    - name: Attorney_ID\\n      data_type: int\\n      transform:\\n        source: etl\\n        source_entity_type: form\\n        source_entity_name: casesummary\\n        source_field: primaryattorney\\n        type: data\\n    - name: Prelitigation_Paralegal_ID\\n      data_type: int\\n      transform:\\n        source: etl\\n        source_entity_type: form\\n        source_entity_name: casesummary\\n        source_field: paralegal\\n        type: data\\n    - name: Litigation_Paralegal_ID\\n      data_type: int\\n      transform:\\n        source: etl\\n        source_entity_type: form\\n        source_entity_name: casesummary\\n        source_field: paralegal\\n        type: data                \\n    - name: CaseManager_ID\\n      data_type: int\\n      transform:\\n        source: etl\\n        source_entity_type: form\\n        source_entity_name: casesummary\\n        source_field: casemanager\\n        type: data\\n    - name: Cocounsel_ID\\n      data_type: int\\n      transform:\\n        source: etl\\n        source_entity_type: form\\n        source_entity_name: casesummary\\n        source_field: cocounsel\\n        type: data    \\n    - name: Case_Team_ID\\n      data_type: int\\n    - name: Insurance_ID\\n      data_type: int\\n    - name: Case_Status_ID\\n      data_type: int\\n    - name: Case_Marketing_Source\\n      data_type: int\\n    - name: Case_Source_Name\\n      data_type: string\\n    - name: Attorney_Fee_Percentage\\n      data_type: float\\n      transform:\\n        source: etl\\n        source_entity_type: form\\n        source_entity_name: casesummary\\n        source_field: attorneysFeesAmount\\n        type: data    \\n    - name: Projected_Settlement_Date\\n      data_type: date\\n      transform:\\n        source: etl\\n        source_entity_type: form\\n        source_entity_name: casesummary\\n        source_field: projectedsettlementdate\\n        type: data    \\n    - name: Projected_Settlement_Amount\\n      data_type: float\\n      transform:\\n        source: etl\\n        source_entity_type: form\\n        source_entity_name: casesummary\\n        source_field: projectedsettlementgoal\\n        type: data\\n    - name: Actual_Settlement_Date\\n      data_type: date\\n      transform:\\n        source: etl\\n        source_entity_type: form\\n        source_entity_name: casesummary\\n        source_field: settlementdate\\n        type: data    \\n    - name: Actual_Settlement_Amount\\n      data_type: float\\n      transform:\\n        source: etl\\n        source_entity_type: form\\n        source_entity_name: casesummary\\n        source_field: actualsettlementamount\\n        type: data\\n    - name: If_Case_Settled_Presuit\\n      data_type: boolean\\n      transform:\\n        source: etl\\n        source_entity_type: form\\n        source_entity_name: casesummary\\n        source_field: caseSettledPresuit\\n        type: data\\n    - name: If_VIP_Case\\n      data_type: boolean\\n      transform:\\n        source: etl\\n        source_entity_type: form\\n        source_entity_name: casesummary\\n        source_field: vIPCase\\n        type: data\\n    - name: If_Case_Referred_Out\\n      data_type: boolean\\n    - name: Case_Phase_ID\\n      data_type: int\\n    - name: Custom1\\n      data_type: string  \\n    - name: Custom2\\n      data_type: string  \\n    - name: Custom3\\n      data_type: string      \\n    - name: Custom4\\n      data_type: string      \\n    - name: Custom5\\n      data_type: string      '\n"
     ]
    }
   ],
   "source": [
    "sstm_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "826bfe0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def readFromFile(ext, path) :\n",
    "    fileContent = spark.read.format(ext).option(\"header\", \"true\").load(path)\n",
    "    return fileContent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "088295e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def writeToFile(df_join , path, mode) :\n",
    "    df_join.write.parquet(path,mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffc29d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\",\"false\")\n",
    "spark.conf.set(\"spark.sql.sources.partitionColumnTypeInference.enabled\", \"false\")\n",
    "spark.conf.set(\"spark.sql.parquet.writeLegacyFormat\", \"true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c402ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sstm_transformation.tsm_builder import TSMBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65ab263f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#PeopleType Transformation\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "builder = TSMBuilder(\"sstm.yaml\", spark=spark)\n",
    "def people_type_transformation():\n",
    "    #Read Raw Data\n",
    "    contact_dyf = glueContext.create_dynamic_frame_from_options(\\\n",
    "    connection_type = \"s3\", \\\n",
    "    connection_options = {\n",
    "        \"paths\": [\"s3://dev-truve-devops-05-databricks-bucketetlrawdata-wu3m2thgf3o/filevine/6586/contact/historical_contacts.parquet\"]}, \\\n",
    "    format = \"parquet\",\n",
    "    )\n",
    "    contact_sp_df = contact_dyf.toDF()\n",
    "    sstm_peoplemaster_df = builder.build_peopletypes(contact_df=contact_sp_df)\n",
    "    sstm_peoplemaster_dyf = DynamicFrame.fromDF(sstm_peoplemaster_df, glueContext, \"sstm_peoplemaster\")\n",
    "    print(sstm_peoplemaster_dyf.count())\n",
    "    sstm_peoplemaster_df.write.mode(\"overwrite\").format(\"parquet\").save(\"s3://dev-truve-devops-05-databr-bucketetlprocesseddata-h2m2xopoctot/peoplemaster/\")\n",
    "    print(\"PeopleTypes Processed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e015b833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def people_master_transformation():\n",
    "    #Read Raw Data\n",
    "    contact_dyf = glueContext.create_dynamic_frame_from_options(\\\n",
    "    connection_type = \"s3\", \\\n",
    "    connection_options = {\n",
    "        \"paths\": [\"s3://dev-truve-devops-05-databricks-bucketetlrawdata-wu3m2thgf3o/filevine/6586/contact/historical_contacts.parquet\"]}, \\\n",
    "    format = \"parquet\",\n",
    "    )\n",
    "    contact_sp_df = contact_dyf.toDF()\n",
    "    print(contact_sp_df.count())\n",
    "    sstm_peoplemaster_df = builder.build_peoplemaster(contact_df=contact_sp_df)\n",
    "    trans_obj = DynamicFrame.fromDF(sstm_peoplemaster_df, glueContext, \"sstm_peoplemaster\")\n",
    "    print(trans_obj)\n",
    "    #sstm_peoplemaster_df.write.mode(\"overwrite\").format(\"parquet\").save(\"s3://dev-truve-devops-05-databr-bucketetlprocesseddata-h2m2xopoctot/peoplemaster/\")\n",
    "    #print(\"PeopleTypes Processed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58b6a7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnalysisException: 'Path does not exist: file:/home/ubuntu/freelancer/scylla/data-api/sstm_input_data/historical_contacts.parquet;'\n"
     ]
    }
   ],
   "source": [
    "#people_type_transformation()\n",
    "people_master_transformation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9242600f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_forms =readFromFile(\"parquet\", \"s3://dev-truve-devops-05-databricks-bucketetlrawdata-wu3m2thgf3o/filevine/6586/18764/*/form/casesummary.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d99540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "518d9b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1198\n"
     ]
    }
   ],
   "source": [
    "df_forms.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "397dc58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- judge: string (nullable = true)\n",
      " |-- statuteoflimitations: string (nullable = true)\n",
      " |-- cocounsel: string (nullable = true)\n",
      " |-- complaintfiled: date (nullable = true)\n",
      " |-- paralegal: string (nullable = true)\n",
      " |-- primaryattorney: string (nullable = true)\n",
      " |-- settlementdate: date (nullable = true)\n",
      " |-- courtdate: date (nullable = true)\n",
      " |-- projectedsettlementdate: date (nullable = true)\n",
      " |-- projectedsettlementgoal: string (nullable = true)\n",
      " |-- actualsettlementamount: string (nullable = true)\n",
      " |-- casefeesamount: string (nullable = true)\n",
      " |-- casefeesdetails: string (nullable = true)\n",
      " |-- clientdispersaldate: date (nullable = true)\n",
      " |-- demandsent: date (nullable = true)\n",
      " |-- caseType: string (nullable = true)\n",
      " |-- vIPCase: boolean (nullable = true)\n",
      " |-- venue: string (nullable = true)\n",
      " |-- descriptionOfIncident: string (nullable = true)\n",
      " |-- 100LiabilityAdmitted: boolean (nullable = true)\n",
      " |-- why: string (nullable = true)\n",
      " |-- propertyDamage: string (nullable = true)\n",
      " |-- policeReportRcvd: date (nullable = true)\n",
      " |-- spoliationToTruckingCo: date (nullable = true)\n",
      " |-- anteLitemSentToCityCounty: date (nullable = true)\n",
      " |-- clientStatusOnProperty: string (nullable = true)\n",
      " |-- natureOfHazard: string (nullable = true)\n",
      " |-- returnReceiptDate: date (nullable = true)\n",
      " |-- surveillanceFootage: boolean (nullable = true)\n",
      " |-- anteLitemReceived: date (nullable = true)\n",
      " |-- spoliationToPropertyOwnerM: date (nullable = true)\n",
      " |-- spoliationReceived: date (nullable = true)\n",
      " |-- positiveLiabilityFactors: string (nullable = true)\n",
      " |-- defendantCited: boolean (nullable = true)\n",
      " |-- businessLicenseOrdered: date (nullable = true)\n",
      " |-- businessLicenseReceived: date (nullable = true)\n",
      " |-- dateOfIncident: date (nullable = true)\n",
      " |-- referralSource: string (nullable = true)\n",
      " |-- amountWithheldInIOLTA: string (nullable = true)\n",
      " |-- discoveryExpires: date (nullable = true)\n",
      " |-- discoveryExtended: boolean (nullable = true)\n",
      " |-- discoveryExtendedThrough: date (nullable = true)\n",
      " |-- court: string (nullable = true)\n",
      " |-- caseStyle: string (nullable = true)\n",
      " |-- civilActionFileNo: string (nullable = true)\n",
      " |-- staffAttorneyAdminContact: string (nullable = true)\n",
      " |-- generalStandingOrderOnFile: boolean (nullable = true)\n",
      " |-- filingVendor: string (nullable = true)\n",
      " |-- descriptionOfInjuries: string (nullable = true)\n",
      " |-- surgicalRecommendation: boolean (nullable = true)\n",
      " |-- surgeryPerformed: boolean (nullable = true)\n",
      " |-- procedure: string (nullable = true)\n",
      " |-- dateOfSurgery: date (nullable = true)\n",
      " |-- impairmentRating: string (nullable = true)\n",
      " |-- providerAssigningImpairment: string (nullable = true)\n",
      " |-- citationDisposition: string (nullable = true)\n",
      " |-- requestCertifiedCopy: boolean (nullable = true)\n",
      " |-- liability1PolicyLimits: string (nullable = true)\n",
      " |-- liabilityCarrier2: string (nullable = true)\n",
      " |-- liability2PolicyLimits: string (nullable = true)\n",
      " |-- liability2Limits: string (nullable = true)\n",
      " |-- uM1PolicyLimits: string (nullable = true)\n",
      " |-- uM1Type: string (nullable = true)\n",
      " |-- uMCarrier2: string (nullable = true)\n",
      " |-- uM2PolicyLimits: string (nullable = true)\n",
      " |-- uM2Limits: string (nullable = true)\n",
      " |-- uM2Type: string (nullable = true)\n",
      " |-- medpayLimits: string (nullable = true)\n",
      " |-- medpayOtherLimitsAmmt: string (nullable = true)\n",
      " |-- healthInsurance: string (nullable = true)\n",
      " |-- validRightToSubro: boolean (nullable = true)\n",
      " |-- priorClaimsInfo: string (nullable = true)\n",
      " |-- diagnostics: string (nullable = true)\n",
      " |-- procedures: string (nullable = true)\n",
      " |-- providerType: string (nullable = true)\n",
      " |-- surgery2: string (nullable = true)\n",
      " |-- dateOfSurgery2: date (nullable = true)\n",
      " |-- surgery3: string (nullable = true)\n",
      " |-- dateOfSurgery3: date (nullable = true)\n",
      " |-- impairmentRatingAssigned: boolean (nullable = true)\n",
      " |-- impairmentRatingRequested: boolean (nullable = true)\n",
      " |-- dateIRRequested: date (nullable = true)\n",
      " |-- coCounsel_1: boolean (nullable = true)\n",
      " |-- feeSplit: double (nullable = true)\n",
      " |-- caseSettledPresuit: boolean (nullable = true)\n",
      " |-- lawSuitFiled: boolean (nullable = true)\n",
      " |-- liabilityCarrier1: string (nullable = true)\n",
      " |-- isThereMoreThanOneLiabili: boolean (nullable = true)\n",
      " |-- liabilityAcceptedByCarrier: boolean (nullable = true)\n",
      " |-- listPolicyLimitsForCarrier: string (nullable = true)\n",
      " |-- hasLiabilityInsurance2Acce: boolean (nullable = true)\n",
      " |-- whyIsLiabilityInsurance1N: string (nullable = true)\n",
      " |-- whyHasLiabilityInsurance2: string (nullable = true)\n",
      " |-- isThereMoreThan1UMPolicy: boolean (nullable = true)\n",
      " |-- doesClientHaveMedpay: boolean (nullable = true)\n",
      " |-- doesClientHaveHealthInsura: boolean (nullable = true)\n",
      " |-- didClientHaveMoreThanOne: boolean (nullable = true)\n",
      " |-- didClientHaveMoreThanTwo: boolean (nullable = true)\n",
      " |-- didClientHaveMoreThanThre: boolean (nullable = true)\n",
      " |-- listAnyOtherSurgeriesAndT: string (nullable = true)\n",
      " |-- whatSurgeryWasRecommended: string (nullable = true)\n",
      " |-- possibleInsuranceRedFlagsF: string (nullable = true)\n",
      " |-- describeTheLiabilityDispute: string (nullable = true)\n",
      " |-- anteLitemNotice: integer (nullable = true)\n",
      " |-- anteLitemReturnReceipt: integer (nullable = true)\n",
      " |-- companionCaseS: string (nullable = true)\n",
      " |-- truckingSpoliationLtr: integer (nullable = true)\n",
      " |-- truckingCoReturnReceipt: integer (nullable = true)\n",
      " |-- spoliationToPremisesOwner: integer (nullable = true)\n",
      " |-- returnReceiptFromPremisesO: integer (nullable = true)\n",
      " |-- businessLicenseForPremises: integer (nullable = true)\n",
      " |-- defendant1ServedOn: date (nullable = true)\n",
      " |-- defendant1DefaultDate: date (nullable = true)\n",
      " |-- defendant1AnswerFiledOn: date (nullable = true)\n",
      " |-- moreThan1Defendant: boolean (nullable = true)\n",
      " |-- defendant2ServedOn: date (nullable = true)\n",
      " |-- defendant2AnswerFiledOn: date (nullable = true)\n",
      " |-- moreThan2Defendants: boolean (nullable = true)\n",
      " |-- defendant3ServedOn: date (nullable = true)\n",
      " |-- defendant3DefaultDate: date (nullable = true)\n",
      " |-- defendant3AnswerFiledOn: date (nullable = true)\n",
      " |-- defendant2DefaultDate: date (nullable = true)\n",
      " |-- moreThan3Defendants: boolean (nullable = true)\n",
      " |-- pleaseListAdditionalDefenda: string (nullable = true)\n",
      " |-- expertsDisclosedBy: date (nullable = true)\n",
      " |-- dispositiveMotionsFiledBy: date (nullable = true)\n",
      " |-- rebuttalExpertsIdentifiedBy: date (nullable = true)\n",
      " |-- consolidatedPreTrialOrderD: date (nullable = true)\n",
      " |-- motionsInLimineRequestTo: date (nullable = true)\n",
      " |-- trialDate: date (nullable = true)\n",
      " |-- initialParalegal: string (nullable = true)\n",
      " |-- uninsuredMotoristUM1: string (nullable = true)\n",
      " |-- impairmentRatingPercentage: string (nullable = true)\n",
      " |-- spouse: string (nullable = true)\n",
      " |-- children: boolean (nullable = true)\n",
      " |-- currentlyInBankruptcy: boolean (nullable = true)\n",
      " |-- dateBankruptcyFilesChapte: string (nullable = true)\n",
      " |-- bankruptcyAttorney: string (nullable = true)\n",
      " |-- doYouCurrentlyOweChildSup: boolean (nullable = true)\n",
      " |-- amountOwed: string (nullable = true)\n",
      " |-- cityStateChildSupportFiled: string (nullable = true)\n",
      " |-- isTheTitleOfTheVehicleIn: boolean (nullable = true)\n",
      " |-- whoseVehicleWereYouIn: string (nullable = true)\n",
      " |-- vehicleOwnerSRelationshipT: string (nullable = true)\n",
      " |-- wereYouDriving: boolean (nullable = true)\n",
      " |-- whoWasDriving: string (nullable = true)\n",
      " |-- driversRelationshipToClient: string (nullable = true)\n",
      " |-- clientsVehicleYearMakeAn: string (nullable = true)\n",
      " |-- uM1ClaimNumber: string (nullable = true)\n",
      " |-- uM1ClaimsAdjuster: string (nullable = true)\n",
      " |-- isLiabilityVehicleDifferent: boolean (nullable = true)\n",
      " |-- additionalNotesOnAtFaultV: string (nullable = true)\n",
      " |-- liabilityClaimNumber: string (nullable = true)\n",
      " |-- photosOfAtFaultVehicle: boolean (nullable = true)\n",
      " |-- photosOfVehicleClientWasI: boolean (nullable = true)\n",
      " |-- photosOfClientsInjuries: boolean (nullable = true)\n",
      " |-- anyPriorInjuries: boolean (nullable = true)\n",
      " |-- describePriorInjuries: string (nullable = true)\n",
      " |-- anyPriorInjuryClaims: boolean (nullable = true)\n",
      " |-- priorInjuryClaimDetailsAt: string (nullable = true)\n",
      " |-- anyPriorLawsuits: boolean (nullable = true)\n",
      " |-- describePriorLawsuits: string (nullable = true)\n",
      " |-- employmentStatus: string (nullable = true)\n",
      " |-- clientsHourlyRateSalary: string (nullable = true)\n",
      " |-- employer: string (nullable = true)\n",
      " |-- lostWagesStart: date (nullable = true)\n",
      " |-- lostWagesEnd: date (nullable = true)\n",
      " |-- jobTitle: string (nullable = true)\n",
      " |-- jobDuties: string (nullable = true)\n",
      " |-- concurrentEmployment2Job: boolean (nullable = true)\n",
      " |-- concurrentEmployer1: string (nullable = true)\n",
      " |-- cC1HourlyRateSalary: string (nullable = true)\n",
      " |-- cC1Employer: string (nullable = true)\n",
      " |-- liabilityAdjusterSName: string (nullable = true)\n",
      " |-- doYouHaveAnyRelativesWho: boolean (nullable = true)\n",
      " |-- listAllRelativesWhoDrive: string (nullable = true)\n",
      " |-- doYouLiveAnywhereElse: boolean (nullable = true)\n",
      " |-- listAllOtherAddressesThat: string (nullable = true)\n",
      " |-- doYouGetMailAtAnyOtherH: boolean (nullable = true)\n",
      " |-- listAnyOtherAddressesWhere: string (nullable = true)\n",
      " |-- doYouHaveAClosetOrBedroo: boolean (nullable = true)\n",
      " |-- listAllOtherAddressesWhere: string (nullable = true)\n",
      " |-- whatAreSomeThingsYouCanN: string (nullable = true)\n",
      " |-- whatAreSomeThingsYouCanD: string (nullable = true)\n",
      " |-- howHasYourLifeBeenAffecte: string (nullable = true)\n",
      " |-- anySubsequentAccidentsOrIn: boolean (nullable = true)\n",
      " |-- explanation: string (nullable = true)\n",
      " |-- healthInsuranceMemberID: string (nullable = true)\n",
      " |-- healthInsuranceGroupNumber: string (nullable = true)\n",
      " |-- bodyPartSAffected: string (nullable = true)\n",
      " |-- listNamesAndAges: string (nullable = true)\n",
      " |-- caseManager: string (nullable = true)\n",
      " |-- hasSettlementCheckBeenRece: boolean (nullable = true)\n",
      " |-- settlementCheckAmount: string (nullable = true)\n",
      " |-- typeOfSettlement: string (nullable = true)\n",
      " |-- areThereAdditionalSettlemen: boolean (nullable = true)\n",
      " |-- secondSettlementType: string (nullable = true)\n",
      " |-- secondSettlementAmount: string (nullable = true)\n",
      " |-- isThereAThirdSettlement: boolean (nullable = true)\n",
      " |-- haveAttorneysFeesBeenRecei: boolean (nullable = true)\n",
      " |-- attorneysFeesAmount: string (nullable = true)\n",
      " |-- typeOfAttorneysFee: string (nullable = true)\n",
      " |-- additionalAttorneysFeesPai: string (nullable = true)\n",
      " |-- typeOfAdditionalAttorneys: string (nullable = true)\n",
      " |-- thirdSettlementType: string (nullable = true)\n",
      " |-- thirdSettlementAmount: string (nullable = true)\n",
      " |-- dateSettlementCheckDeposite: date (nullable = true)\n",
      " |-- dateAttorneyFeesCollected: date (nullable = true)\n",
      " |-- secondSettlementDateAttorn: date (nullable = true)\n",
      " |-- hasSecondSettlementCheckBe: boolean (nullable = true)\n",
      " |-- secondSettlementHaveAttorn: boolean (nullable = true)\n",
      " |-- secondSettlementDateCheck: date (nullable = true)\n",
      " |-- hasThirdSettlementCheckBee: boolean (nullable = true)\n",
      " |-- thirdSettlementDateCheckR: date (nullable = true)\n",
      " |-- thirdSettlementHaveAttorne: boolean (nullable = true)\n",
      " |-- thirdSettlementAttorneysF: string (nullable = true)\n",
      " |-- thirdSettlementTypeOfAddi: string (nullable = true)\n",
      " |-- thirdSettlementDateAttorne: date (nullable = true)\n",
      " |-- disbursementSheetSigned: date (nullable = true)\n",
      " |-- clientReview: date (nullable = true)\n",
      " |-- mRIDone: boolean (nullable = true)\n",
      " |-- reviewGiven: date (nullable = true)\n",
      " |-- reviewedForFunding: boolean (nullable = true)\n",
      " |-- fundedBy: string (nullable = true)\n",
      " |-- googleReview: boolean (nullable = true)\n",
      " |-- dateOfReview: date (nullable = true)\n",
      " |-- byWhom: string (nullable = true)\n",
      " |-- projectId: integer (nullable = true)\n"
     ]
    }
   ],
   "source": [
    "df_forms.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3dfcde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Py4JJavaError: An error occurred while calling o402.showString.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 31.0 failed 4 times, most recent failure: Lost task 0.3 in stage 31.0 (TID 6362, 172.35.84.28, executor 1): org.apache.spark.sql.execution.QueryExecutionException: Encounter error while reading parquet files. One possible cause: Parquet column cannot be converted in the corresponding files. Details: \n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:193)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 0 in block -1 in file s3://dev-truve-devops-05-databricks-bucketetlrawdata-wu3m2thgf3o/filevine/6586/18764/8735116/form/casesummary.parquet\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:251)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:207)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:181)\n",
      "\t... 21 more\n",
      "Caused by: java.lang.ClassCastException: Expected instance of group converter but got \"org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter\"\n",
      "\tat org.apache.parquet.io.api.Converter.asGroupConverter(Converter.java:34)\n",
      "\tat org.apache.parquet.io.RecordReaderImplementation.<init>(RecordReaderImplementation.java:267)\n",
      "\tat org.apache.parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:147)\n",
      "\tat org.apache.parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:109)\n",
      "\tat org.apache.parquet.filter2.compat.FilterCompat$NoOpFilter.accept(FilterCompat.java:165)\n",
      "\tat org.apache.parquet.io.MessageColumnIO.getRecordReader(MessageColumnIO.java:109)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordReader.checkRead(InternalParquetRecordReader.java:137)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:222)\n",
      "\t... 25 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n",
      "\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n",
      "\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n",
      "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n",
      "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.sql.execution.QueryExecutionException: Encounter error while reading parquet files. One possible cause: Parquet column cannot be converted in the corresponding files. Details: \n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:193)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 0 in block -1 in file s3://dev-truve-devops-05-databricks-bucketetlrawdata-wu3m2thgf3o/filevine/6586/18764/8735116/form/casesummary.parquet\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:251)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:207)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:181)\n",
      "\t... 21 more\n",
      "Caused by: java.lang.ClassCastException: Expected instance of group converter but got \"org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter\"\n",
      "\tat org.apache.parquet.io.api.Converter.asGroupConverter(Converter.java:34)\n",
      "\tat org.apache.parquet.io.RecordReaderImplementation.<init>(RecordReaderImplementation.java:267)\n",
      "\tat org.apache.parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:147)\n",
      "\tat org.apache.parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:109)\n",
      "\tat org.apache.parquet.filter2.compat.FilterCompat$NoOpFilter.accept(FilterCompat.java:165)\n",
      "\tat org.apache.parquet.io.MessageColumnIO.getRecordReader(MessageColumnIO.java:109)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordReader.checkRead(InternalParquetRecordReader.java:137)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:222)\n",
      "\t... 25 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_forms.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12b12b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from awsglue.dynamicframe import DynamicFrame\n",
    "\n",
    "dyf_casesummary = DynamicFrame.fromDF(df_forms, glueContext, \"nested\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51af2d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Py4JJavaError: An error occurred while calling o160.pyWriteDynamicFrame.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "\tat com.amazonaws.services.glue.SparkSQLDataSink$$anonfun$writeDynamicFrame$1.apply(DataSink.scala:602)\n",
      "\tat com.amazonaws.services.glue.SparkSQLDataSink$$anonfun$writeDynamicFrame$1.apply(DataSink.scala:589)\n",
      "\tat com.amazonaws.services.glue.util.FileSchemeWrapper$$anonfun$executeWithQualifiedScheme$1.apply(FileSchemeWrapper.scala:89)\n",
      "\tat com.amazonaws.services.glue.util.FileSchemeWrapper$$anonfun$executeWithQualifiedScheme$1.apply(FileSchemeWrapper.scala:89)\n",
      "\tat com.amazonaws.services.glue.util.FileSchemeWrapper.executeWith(FileSchemeWrapper.scala:82)\n",
      "\tat com.amazonaws.services.glue.util.FileSchemeWrapper.executeWithQualifiedScheme(FileSchemeWrapper.scala:89)\n",
      "\tat com.amazonaws.services.glue.SparkSQLDataSink.writeDynamicFrame(DataSink.scala:588)\n",
      "\tat com.amazonaws.services.glue.DataSink.pyWriteDynamicFrame(DataSink.scala:65)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 4 times, most recent failure: Lost task 0.3 in stage 10.0 (TID 1383, 172.35.84.28, executor 1): org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.UnsupportedOperationException: Complex types not supported.\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initializeInternal(VectorizedParquetRecordReader.java:278)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initialize(VectorizedParquetRecordReader.java:132)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1.apply(ParquetFileFormat.scala:418)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1.apply(ParquetFileFormat.scala:352)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:244)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)\n",
      "\t... 10 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n",
      "\t... 39 more\n",
      "Caused by: org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: java.lang.UnsupportedOperationException: Complex types not supported.\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initializeInternal(VectorizedParquetRecordReader.java:278)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initialize(VectorizedParquetRecordReader.java:132)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1.apply(ParquetFileFormat.scala:418)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1.apply(ParquetFileFormat.scala:352)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:244)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)\n",
      "\t... 10 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "glueContext.write_dynamic_frame.from_options(frame = dyf_casesummary,\n",
    "                                         connection_type = \"s3\",\n",
    "                                         connection_options = {\"path\": \"s3://dev-truve-devops-05-databricks-bucketetlrawdata-wu3m2thgf3o/temp/casesummary/\"},\n",
    "                                         format = \"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a922154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Py4JJavaError: An error occurred while calling o121.parquet.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 4 times, most recent failure: Lost task 0.3 in stage 9.0 (TID 1362, 172.35.84.28, executor 1): org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.UnsupportedOperationException: Complex types not supported.\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initializeInternal(VectorizedParquetRecordReader.java:278)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initialize(VectorizedParquetRecordReader.java:132)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1.apply(ParquetFileFormat.scala:418)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1.apply(ParquetFileFormat.scala:352)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:244)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)\n",
      "\t... 10 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n",
      "\t... 33 more\n",
      "Caused by: org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: java.lang.UnsupportedOperationException: Complex types not supported.\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initializeInternal(VectorizedParquetRecordReader.java:278)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initialize(VectorizedParquetRecordReader.java:132)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1.apply(ParquetFileFormat.scala:418)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1.apply(ParquetFileFormat.scala:352)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:244)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)\n",
      "\t... 10 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "writeToFile(df_forms,\"s3://dev-truve-devops-05-databricks-bucketetlrawdata-wu3m2thgf3o/temp/casesummary/\", \"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d30e2e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping session: 9c969628-1cdb-428c-8730-7423bf060195\n",
      "Stopped session.\n"
     ]
    }
   ],
   "source": [
    "%stop_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db45956f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_forms =readFromFile(\"parquet\", \"s3://dev-truve-devops-05-databricks-bucketetlrawdata-wu3m2thgf3o/filevine/6586/18764/*/collection/meds.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c65ea363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6872\n"
     ]
    }
   ],
   "source": [
    "df_forms.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ebad426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- provider: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- isThisALien: boolean (nullable = true)\n",
      " |-- valueTotal: string (nullable = true)\n",
      " |-- balanceDue: string (nullable = true)\n",
      " |-- projectId: integer (nullable = true)\n",
      " |-- serviceDateFrom: date (nullable = true)\n",
      " |-- serviceDateTo: date (nullable = true)\n",
      " |-- referenceFileNumber: string (nullable = true)\n",
      " |-- payment1: string (nullable = true)\n",
      " |-- notes: string (nullable = true)\n",
      " |-- paidBy1: string (nullable = true)\n",
      " |-- haveallbillsbeenordered: boolean (nullable = true)\n",
      " |-- billsordereddate: date (nullable = true)\n",
      " |-- haveallrecordsbeenordered: boolean (nullable = true)\n",
      " |-- haveallrecordsbeenreceived: boolean (nullable = true)\n",
      " |-- recordsinvoicereceived: boolean (nullable = true)\n",
      " |-- howDidWeConnectWithProvid: string (nullable = true)\n",
      " |-- reviewedForFunding: boolean (nullable = true)\n",
      " |-- snagUpdates: string (nullable = true)\n",
      " |-- recordsordereddate: date (nullable = true)\n",
      " |-- haveallbillsbeenreceived: boolean (nullable = true)\n",
      " |-- billinvoicereceived: boolean (nullable = true)\n",
      " |-- totalReduction_1: string (nullable = true)\n"
     ]
    }
   ],
   "source": [
    "df_forms.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6352205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----------+----------+--------------------+---------+---------------+-------------+--------------------+--------+--------------------+-------------------+-----------------------+----------------+-------------------------+--------------------------+----------------------+-------------------------+------------------+--------------------+------------------+------------------------+-------------------+----------------+\n",
      "|provider|         type|isThisALien|valueTotal|          balanceDue|projectId|serviceDateFrom|serviceDateTo| referenceFileNumber|payment1|               notes|            paidBy1|haveallbillsbeenordered|billsordereddate|haveallrecordsbeenordered|haveallrecordsbeenreceived|recordsinvoicereceived|howDidWeConnectWithProvid|reviewedForFunding|         snagUpdates|recordsordereddate|haveallbillsbeenreceived|billinvoicereceived|totalReduction_1|\n",
      "+--------+-------------+-----------+----------+--------------------+---------+---------------+-------------+--------------------+--------+--------------------+-------------------+-----------------------+----------------+-------------------------+--------------------------+----------------------+-------------------------+------------------+--------------------+------------------+------------------------+-------------------+----------------+\n",
      "|33069556|Case Expenses|       null|     72.88|{'value': 0.0, 'e...|  9675645|     2022-06-25|         null|SNAG INV - Physic...|   72.88|                 -AT|SNAG IBS AND PI FEE|                   null|            null|                     null|                      null|                  null|                     null|              null|                null|              null|                    null|               null|            null|\n",
      "|33069556|Case Expenses|       null|      35.0|{'value': 0.0, 'e...|  9675645|     2022-06-24|         null|SNAG INV - Radiol...|    35.0|                 -CC|       Snag Records|                   null|            null|                     null|                      null|                  null|                     null|              null|                null|              null|                    null|               null|            null|\n",
      "|23903621|Case Expenses|       null|     37.88|{'value': 0.0, 'e...|  9675645|     2022-06-24|         null|CHARTSWAP - Radio...|   37.88|                 -CC|   CHARTSWAP - AMEX|                   null|            null|                     null|                      null|                  null|                     null|              null|                null|              null|                    null|               null|            null|\n",
      "|34676000| Medical Bill|       null|      null|{'value': None, '...|  9675645|     2021-07-09|   2022-06-23|                null|    null|06/23/2022-Sent r...|               null|                   true|      2022-06-23|                     true|                      null|                  null|     Client chose this...|              null|SR - 7.11.22 Requ...|        2022-06-23|                    null|               null|            null|\n",
      "|34679275| Medical Bill|       null|    2867.0|{'value': 2867.0,...|  9675645|     2021-07-09|   2022-06-23|                null|    null|06/23/22-Sent req...|               null|                   true|      2022-06-23|                     null|                      null|                  null|     Client chose this...|              null|SR - 6.24.22 Resu...|              null|                    null|               null|            null|\n",
      "|23903916| Medical Bill|      false|       0.0|{'value': 0.0, 'e...|  9675645|           null|         null|                null|    null|06/23/22-Drove he...|               null|                   null|            null|                     null|                      null|                  null|                    Other|              null|                null|              null|                    null|               null|            null|\n",
      "|33069556|Case Expenses|       null|      35.0|{'value': 0.0, 'e...|  9675645|     2022-06-14|         null|SNAG INV - St. Jo...|    35.0|                 -CC|       Snag Records|                   null|            null|                     null|                      null|                  null|                     null|              null|                null|              null|                    null|               null|            null|\n",
      "|29704132|Case Expenses|       null|     68.52|{'value': 0.0, 'e...|  9675645|     2022-06-14|         null|MRO - St. Joseph'...|   68.52|                 -CC|         MRO - AMEX|                   null|            null|                     null|                      null|                  null|                     null|              null|                null|              null|                    null|               null|            null|\n",
      "|33069556|Case Expenses|       null|      35.0|{'value': 0.0, 'e...|  9675645|     2022-06-10|         null|SNAG INV - Memori...|    35.0|                 -CC|       Snag Records|                   null|            null|                     null|                      null|                  null|                     null|              null|                null|              null|                    null|               null|            null|\n",
      "|23903978|Case Expenses|       null|    118.96|{'value': 0.0, 'e...|  9675645|     2022-06-10|         null|CIOX - Memorial H...|  118.96|                 -CC|        CIOX - AMEX|                   null|            null|                     null|                      null|                  null|                     null|              null|                null|              null|                    null|               null|            null|\n",
      "+--------+-------------+-----------+----------+--------------------+---------+---------------+-------------+--------------------+--------+--------------------+-------------------+-----------------------+----------------+-------------------------+--------------------------+----------------------+-------------------------+------------------+--------------------+------------------+------------------------+-------------------+----------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "df_forms.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2cc03f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "writeToFile(df_forms,\"s3://dev-truve-devops-05-databricks-bucketetlrawdata-wu3m2thgf3o/temp/meds/\", \"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33deb8bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Glue PySpark",
   "language": "python",
   "name": "glue_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "Python_Glue_Session",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
